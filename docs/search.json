[
  {
    "objectID": "user_policy.html",
    "href": "user_policy.html",
    "title": "Bioinformatics servers and user policy",
    "section": "",
    "text": "These servers are funded by the Wellcome Trust and are managed by the DRP-HCB bioinformatics core.\nAccounts are limited to users with active DRP-HCB projects. To request an account please contact the bioinformatics core.\nOnce your account is active, the following servers can be accessed over the university network or via the VPN service by logging in with your EASE credentials.\n\nbifx-core1.bio.ed.ac.uk\n\n512Gb RAM\n128 dual CPU cores\nCommand line usage (ssh)\n37Tb scratch space at /scratch\n\nbifx-core2.bio.ed.ac.uk\n\n512Gb RAM\n32 dual CPU cores\nCommand line usage (ssh)\nR-Studio: https://rstudio-core2.bio.ed.ac.uk/\nR-Shiny: http://bifx-core2.bio.ed.ac.uk:3838/\nApache web server\n\nbifx-core3.bio.ed.ac.uk\n\n1Tb RAM\n128 dual CPU cores\nCommand line usage (ssh)\nR-Studio: https://rstudio-core3.bio.ed.ac.uk/\nR-Shiny: http://bifx-core3.bio.ed.ac.uk:3838/\nJupyter Hub: http://bifx-core3.bio.ed.ac.uk:8888/\nApache web server"
  },
  {
    "objectID": "user_policy.html#bioinformatics-servers",
    "href": "user_policy.html#bioinformatics-servers",
    "title": "Bioinformatics servers and user policy",
    "section": "",
    "text": "These servers are funded by the Wellcome Trust and are managed by the DRP-HCB bioinformatics core.\nAccounts are limited to users with active DRP-HCB projects. To request an account please contact the bioinformatics core.\nOnce your account is active, the following servers can be accessed over the university network or via the VPN service by logging in with your EASE credentials.\n\nbifx-core1.bio.ed.ac.uk\n\n512Gb RAM\n128 dual CPU cores\nCommand line usage (ssh)\n37Tb scratch space at /scratch\n\nbifx-core2.bio.ed.ac.uk\n\n512Gb RAM\n32 dual CPU cores\nCommand line usage (ssh)\nR-Studio: https://rstudio-core2.bio.ed.ac.uk/\nR-Shiny: http://bifx-core2.bio.ed.ac.uk:3838/\nApache web server\n\nbifx-core3.bio.ed.ac.uk\n\n1Tb RAM\n128 dual CPU cores\nCommand line usage (ssh)\nR-Studio: https://rstudio-core3.bio.ed.ac.uk/\nR-Shiny: http://bifx-core3.bio.ed.ac.uk:3838/\nJupyter Hub: http://bifx-core3.bio.ed.ac.uk:8888/\nApache web server"
  },
  {
    "objectID": "user_policy.html#server-access",
    "href": "user_policy.html#server-access",
    "title": "Bioinformatics servers and user policy",
    "section": "Server access",
    "text": "Server access\nFor command line access, use the ssh command from a Terminal application in Windows, MacOS and Linux e.g.\n\nssh -Y username@bifx-core1.bio.ed.ac.uk\n\nIf you prefer to use a graphical interface the servers can be accessed via X2Go or with the Windows Remote Desktop application.\n\nX2Go\n\nDownload the X2Go client\n\nRemote Desktop\n\nPre-installed on Windows\nDownload for Mac from the App store\n\n\n\nR and RStudio\n\nRStudio on bifx-core3 can be accessed at https://rstudio-core3.bio.ed.ac.uk/\nA Shiny server is available on the university network only at http://bifx-core3.bio.ed.ac.uk:3838\n\nAdd your own Shiny Apps to a ShinyApps folder in your home directory.\nLaunch these from http://bifx-core3.bio.ed.ac.uk:3838/username/\n\n\n\n\nJupyter Hub\n\nJupyter Hub on bifx-core3 can be accessed at https://bifx-core3.bio.ed.ac.uk:8888\n\n\n\nWeb server\nData on the server can be accessed over the web (e.g. files, images, html documents, IGV/UCSC tracks). Please contact us for details on how to set this up."
  },
  {
    "objectID": "user_policy.html#data-policy",
    "href": "user_policy.html#data-policy",
    "title": "Bioinformatics servers and user policy",
    "section": "Data policy",
    "text": "Data policy\n\nNo personal data should be stored on the bifx-core servers. All data should be related to projects associated with the DRP unless dispensation has been given.\nAll data on the bifx-core servers can be viewed by members of the bioinformatics core or the DRP-HCB director.\nAs the bioinformatics core team have superuser permissions, we can see all data on the server. Instructions on how to access the servers are lodged with the DRP-HCB director in case of emergency. Therefore they are a de-facto administrator.\nYour data can be accessed by your PI on request. No request is needed if the PI has an account on the server and can view your files. Data stored on the servers should only relate to active DRP-HCB projects submitted by the grant holder and therefore it is reasonable that they can access their own data.\nUsers are responsible for their own data (see backup policy).\nBy default, when you create a new file on the server it is created with the following permissions:\n\nPersonal: read/write/execute\nGlobal/group: read\n\nThis means that only you can change, delete or execute the new file, but anyone logged in to the server can view it. If you would like to change the permissions on a file you can use the chmod command. Further details on how to do this can be found by reviewing our introduction to linux course."
  },
  {
    "objectID": "user_policy.html#data-management",
    "href": "user_policy.html#data-management",
    "title": "Bioinformatics servers and user policy",
    "section": "Data management",
    "text": "Data management\n\nUsers should have a data management plan for their projects. Please see this document for advice.\nUsers should endeavour to minimise disk usage by removing unnecessary files and compressing files where appropriate.\nCompleted projects should be archived with metadata and code to ensure reproducibility.\nBefore you leave the university, please ensure all required data is accessible to your group.\nGroup folders exist on the bifx-core servers at /groups. These can be used for shared data and for archiving old projects/accounts.\nPlease follow our recommended best practices for working on the bifx-core servers. This web page will be updated regularly."
  },
  {
    "objectID": "user_policy.html#server-usage",
    "href": "user_policy.html#server-usage",
    "title": "Bioinformatics servers and user policy",
    "section": "Server usage",
    "text": "Server usage\n\nAll users must complete our introduction to linux course, either in their own time or by attending one of our workshops.\nThe bifx-core servers are a shared environment and users are responsible for ensuring their jobs do not overwhelm computing resources. The servers can cope with high memory usage and multi-threading, however they will slow down when there is a high number of read/write operations.\n\nLimit simultaneous jobs with large read/write requirements (e.g. genome alignment).\nDo not set multi-threaded jobs to use the majority of CPUs.\n\nIn particular, users should be aware of Unix commands to monitor their jobs and avoid overuse of computing resources, e.g:\n\nps / pgrep\ntop / htop\nnice\nkill\n\nUse the scratch space in /scratch to speed up analyses/downloads/uploads. This is local disk space which will respond quicker. However, it is NOT BACKED UP and should be used as temporary storage only. All key datasets should be transferred to your home space."
  },
  {
    "objectID": "user_policy.html#software-and-updates",
    "href": "user_policy.html#software-and-updates",
    "title": "Bioinformatics servers and user policy",
    "section": "Software and updates",
    "text": "Software and updates\n\nThe bioinformatics core will regularly maintain software and perform updates. We will strive to provide adequate warning when updates may cause interruption. All communication will be broadcast via the bifx-users mailing list (see below).\nDetails on version control of bioinformatics software and maintaining your own packages are available in our best practices section."
  },
  {
    "objectID": "user_policy.html#storage-and-backups",
    "href": "user_policy.html#storage-and-backups",
    "title": "Bioinformatics servers and user policy",
    "section": "Storage and Backups",
    "text": "Storage and Backups\n\nData saved in home directories is stored on the University of Edinburgh DataStore service. Two backup systems exist:\n\nDaily ‘snapshots’ kept for two weeks to allow users to restore data.\nDaily tape backups kept for sixty days in case of total data loss.\n\nData in /scratch directories is not backed up. This disk space is provided to speed up analysis but output files should be copied back to your home directory. Please remove data from /scratch regularly to free up space for other users. Old data may be periodically removed by the bioinformatics core facility to free up space.\nWe recommend that users keep additional backups of key datasets e.g:\n\nPersonal copies on encrypted external HDD.\n\nSee here for university advice on encrypting devices.\n\nOnline research data repositories e.g.\n\nSequence repositories (GEO, SRA, ArrayExpress)\nZenodo"
  },
  {
    "objectID": "user_policy.html#mailing-list",
    "href": "user_policy.html#mailing-list",
    "title": "Bioinformatics servers and user policy",
    "section": "Mailing list",
    "text": "Mailing list\nFor announcements regarding server issues and updates, please join our mailing list. To join, send an email with no subject line to the following address: sympa@mlist.is.ed.ac.uk\nInclude the following 2 lines of text (replacing FIRSTNAME and LASTNAME with your own first name and surname) SUBSCRIBE bifx-users@mlist.is.ed.ac.uk FIRSTNAME LASTNAME QUIT"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "DRP-HCB Training Courses",
    "section": "",
    "text": "This page is intended to provide a set of resources for those interested in learning about bioinformatics and acquiring new skills. It is by no means a comprehensive list of online training materials and does not cover everything you need to know to become a skilled bioinformatician. However, it should be enough to get you started and we will happily add more subjects if they are of interest to members of the centre.\nOur dedicated bioinformatics servers provide an expansive list of tools via the Linux command line, an RStudio server and access to programming languages and libraries such as Python and Perl. If you require an account, please contact Shaun Webb."
  },
  {
    "objectID": "training.html#linux-command-line",
    "href": "training.html#linux-command-line",
    "title": "DRP-HCB Training Courses",
    "section": "Linux command line",
    "text": "Linux command line\nLinux operating systems are the workhorse for most large scale genomics applications and are well suited to managing and processing huge data files produced by high throughput sequencing technologies. Although command-line computing can be daunting to new users, grasping a few fundamental commands and learning to run software packages and pipelines from the terminal will open new doors\n\nBeginner\n\nWCB introduction to Linux course\nCommand line cheat-sheet\nThe Unix shell\nCommand line for Genomics\n\n\n\nIntermediate\n\nReproducible research\n\ngit, Conda, Snakemake, R Markdown, Jupyter, Docker and Singularity\n\nPackage management with Conda (coming soon)\n\nGetting started with Conda\nConda cheatsheet\n\nRunning automated pipelines (coming soon)"
  },
  {
    "objectID": "training.html#programming-in-r",
    "href": "training.html#programming-in-r",
    "title": "DRP-HCB Training Courses",
    "section": "Programming in R",
    "text": "Programming in R\nR is a programming language designed for data retrieval, manipulation and visualisation as well as statistical analysis. Learning R will allow you to move beyond spreadsheets and access the full analysis potential of large datasets. You can build multi-dimensional visualisations as well as interactive web applications and documents. It’s easy to get to grips with the basics of R and the RStudio interface provides an intuitive graphical environment in which to develop your code. The extensive Bioconductor libraries also provide pre-built functions to analyse and interpret all sorts of genomic datasets.\n\nBeginner\n\nWCB Introduction to R course\n\nRStudio, base R, Tidyverse packages\n\nR Cheatsheets\n\n\n\nBeginner - Intermediate\n\nR for Data Science\n\n\n\nAdvanced topics\n\nWCB R with genomics data course\nAdvanced R\nR Packages\nWCB Genomic data in R\nR Markdown lessons\nBuild Shiny applications"
  },
  {
    "objectID": "training.html#programming-in-python",
    "href": "training.html#programming-in-python",
    "title": "DRP-HCB Training Courses",
    "section": "Programming in Python",
    "text": "Programming in Python\nPython is a general-purpose programming language that is becoming increasingly popular with data scientists. Python has a growing number of libraries for data manipulation, exploration and visualisation as well as machine learning and is ideal for deploying applications and reproducible code. Jupyter notebooks provide an interactive development environment for coding in Python and other languages.\n\nBeginner - Intermediate\n\nPython data science handbook\n\nAccompanying videos\n\nProgramming with Python\n\n\n\nAdvanced topics\n\nBuilding pipelines with SnakeMake\n\nSnakeMake Tutorial\nSnakeMake slides"
  },
  {
    "objectID": "training.html#statistics-for-biology",
    "href": "training.html#statistics-for-biology",
    "title": "DRP-HCB Training Courses",
    "section": "Statistics for biology",
    "text": "Statistics for biology\nWhile there are countless resources for learning statistics and a wealth of engaging videos on Youtube, it is important for bioinformaticians to understand statistics in the context of biology. Knowledge of statistical analysis will allow you to correctly describe and make inferences from your data as well as designing meaningful and useful experiments. Here are some links to get started:\n\nIntroduction to Stats (slides)\nIntroduction to statistics with R\nPoints of Significance (Nature collection)\nStatistics for biologists (Nature collection)\nStatistical learning (comprehensive stats applied in R)\nRoslin Institute Video Tutorials"
  },
  {
    "objectID": "training.html#analysing-hts-experiments",
    "href": "training.html#analysing-hts-experiments",
    "title": "DRP-HCB Training Courses",
    "section": "Analysing HTS experiments",
    "text": "Analysing HTS experiments\nThe tutorials below provide introductions to analysing specific types of high throughput sequencing experiments. They are designed to help you understand your data, complete generic processing steps and explore and interpret the output. Please note the pre-requisite skills in brackets.\nIn most cases, the pipelines provided are simple and generic and may not be suitable for your own analysis. The bioinformatics core facility is available to collaborate on your project and to offer advice on experimental design and data analysis strategies. We also provide an expansive set of software tools and analysis workflows accessible through our bioinformatics servers:\n\nChIP-seq\n\nWCB ChIP-seq analysis workshop (Linux command line)\n\n\n\nRNA-seq\n\nWCB RNA-seq analysis (Linux command line)\nDifferential expression with DESeq2 (R)\n\n\n\nHi-C\n\nWCB Hi-C data analysis (command line)\n\n\n\nCLIP/CRAC\n\nPyCRAC manual\n\n\n\nCLASH\n\nHyb methods paper\n\n\n\nMethyl-seq\n\nQC and alignment of BS-seq data using Bismark (command line)"
  },
  {
    "objectID": "training.html#further-bioinformatics-training-resources",
    "href": "training.html#further-bioinformatics-training-resources",
    "title": "DRP-HCB Training Courses",
    "section": "Further Bioinformatics Training Resources",
    "text": "Further Bioinformatics Training Resources\nEdinburgh Genomics\n\nTraining programme for Edinburgh Genomics workshops and online courses.\n\nCarpentries\n\nCommunity driven project delivering training in data science and coding skills. Software Carpentry and Data Carpentry websites include many free online courses. Edinburgh Carpentries host regular events at the university and include a Data Carpentry for Genomics course.\n\nHarvardX\n\nA comprehensive set of courses covering R, Python, and biostatistics among other things. Includes case studies in analysing RNA-seq, ChIP-seq and Methyl-seq data.\n\nHarvard Chan Bioinformatics Core\n\nTraining courses in R and command line including many different HTS techniques.\n\nBabraham Bioinformatics\n\nTraining courses offered by the Babraham Bioinformatics group in R, Python, statistics and sequencing applications.\nBitesize bioinformatics modules and videos\n\nRockefeller Bioinformatics\n\nA collection of bioinformatics training courses\n\nBiodatascience\n\nA collection of R courses in data science, statistics and computational biology.\n\nDatacamp\n\nInteractive learning with videos, lessons, tests and a built-in programming interface. Datacamp is a subscription service with some free modules for learning R, Python and data science skills.\n\nUoC bioinformatics core\n\nA collection of bioinformatics training courses from the University of Cambridge bioinformatics core facility."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DRP-HCB-Bioinformatics",
    "section": "",
    "text": "This is the homepage for users of the Bioinformatics Core at the Discovery Research Platform for Hidden Cell Biology at the University of Edinburgh.\n\n\n\nYou will find details on how to access the servers, introductory training course, best-practice documentation for using our in house servers and other useful resources.\nFor more information on how to access or collaborate with the Bioinformatics Core, please see the DRP-HCB website.\n\nFor more information contact Shaun Webb."
  },
  {
    "objectID": "workflows.html",
    "href": "workflows.html",
    "title": "Bioinformatics workflow management tools",
    "section": "",
    "text": "We use two main tools for managing bioinformatics workflows: Snakemake and Nextflow. These tools allow you to define complex workflows in a simple and reproducible way, making it easy to run and share your analyses.\nNextflow is a workflow manager that enables the development of portable and reproducible workflows. Nextflow takes care of managing the execution of the processes and the flow of data between them, making it easy to develop and run complex workflows. The nf-core project provides a collection of curated Nextflow pipelines for bioinformatics analysis. These pipelines are designed to be easy to use, scalable, and reproducible.\nSnakemake is a workflow management system that allows you to define workflows in a simple and readable way using a Python-like syntax. Snakemake automatically determines the order of execution of the rules based on the input and output files, making it easy to parallelize and optimize your workflows. Snakemake is particularly well-suited for bioinformatics workflows, as it integrates seamlessly with popular bioinformatics tools and languages. Snakemake is a good choice for smaller, more focused workflows that do not require the full power of Nextflow. It has a lower learning curve and is easier to get started with."
  },
  {
    "objectID": "workflows.html#nextflow-and-nf-core",
    "href": "workflows.html#nextflow-and-nf-core",
    "title": "Bioinformatics workflow management tools",
    "section": "Nextflow and nf-core",
    "text": "Nextflow and nf-core\nNextflow is a workflow manager that enables the development of portable and reproducible workflows. Nextflow takes care of managing the execution of the processes and the flow of data between them, making it easy to develop and run complex workflows.\n\nInstall Nextflow\n\n#| eval: false\n\n## Install sdk to manage Java\ncurl -s https://get.sdkman.io | bash\n## Log out, then log in again\n## Install java\nsdk install java\n\n## Install Nextflow\ncurl -s https://get.nextflow.io | bash\nchmod +x nextflow\nmkdir -p $HOME/.local/bin/\nmv nextflow $HOME/.local/bin/\n\n## Confirm install\nnextflow info\n\nTo update the version of nextflow:\n#| eval: false\nnextflow self-update\nYou can also temporarily switch to a specific version of Nextflow with the NXF_VER environment variable. For example:\n#| eval: false\n\nNXF_VER=23.10.0 nextflow info\nYou can also use NXF_VER to temporarily switch to any edge release. For example:\n#| eval: false\n\nNXF_VER=24.06.0-edge nextflow info\n\n\nnf-core\nnf-core is a community-driven collection of Nextflow pipelines for bioinformatics. These pipelines are curated and developed by the community, and are designed to be easy to use, scalable, and reproducible. You can find a list of available pipelines at nf-co.re.\nTo install a pipeline from NF-Core, you can use the nextflow pull command. For example, to install the nf-core/rnaseq pipeline:\n#| eval: false\n#| tag: nf-core\n#| file: nf-core\n\nnextflow pull nf-core/rnaseq\nYou can then run the pipeline with the following command:\n#| eval: false\n\nnextflow run nf-core/rnaseq --input samplesheet.csv --genome GRCh38\nSee the nf-core documentation for more information on how to run pipelines.\n\n\nnf-core tools\nNF-Core also provides a set of tools to help you manage your Nextflow pipelines. You can install these tools with conda:\n#| eval: false\n\n## Install with conda\nconda install -c bioconda nf-core"
  },
  {
    "objectID": "downloading.html",
    "href": "downloading.html",
    "title": "downloading",
    "section": "",
    "text": "https://www.youtube.com/watch?v=fAaVzhD7DOU&ab_channel=WCBbioinformatics\nhttps://docs.google.com/presentation/d/12CJ-8CQSryc3K72kok3BHpw-rxTOv2pN06GJF2FP4xE/edit?usp=sharing\n\n\nPublished sequencing data is deposited online in\n\nSRA (Sequence Read Archive)\n\nThe SRA is a repository for raw sequencing data\n\nLong and short reads from any platform (Illumina, PacBio..)\n\nOrganised with unique identifiers\n\nSRP = Project/Study (1 or more samples )\nSRS = Sample (1 or more experiments)\nSRX = Experiment (1 or more runs i.e technical replicates)\nSRR = Run (results for 1 sequencing run)\n\nRuns stored in sra format\n\nReads saved in spots\n\nSmaller datasets and metadata can be downloaded via links in each run\nOther download options\n\nhttps://www.ncbi.nlm.nih.gov/sra/docs/sradownload/\nSRA Toolkit (see below)\nCloud access\n\n\nGEO (Gene Expression Omnibus)\n\nGenomics data repository for high throughput experiments\n\nExperimental metadata\nProcessed data e.g bigWig read profiles, ChIP-seq peak files, RNA-seq read count tables\nLinks to raw data (SRA)\nLinks to publication\n\nData organised into\n\nSeries and super series (GSE)\nSamples (GSM)\n\nMost publications will contain a GEO series ID.\nDownload links are available at the bottom of each GEO submission.\n\nTo download to the server\n\nRight click to “Copy link as”\nUse wget or curl on the command line to download using the URL\n\n\n\nENA (European Nucleotide Archive)\n\nENA is repository for sequencing data hosted by the European Bioinformatics Institute.\nTo download raw sequencing files from a project:\n\nSelect the fastq files you require.\nHit the “Get Download Script” button.\nRun the script on the terminal command line on your computer or on the server.\ne.g. sh ena-file-download-read-run-PRJNA112117-fastq_ftp-20240623-919.sh\n\n\n\n\n\n\n\n\n\n\nhttps://github.com/ncbi/sra-tools/wiki/08.-prefetch-and-fasterq-dump"
  },
  {
    "objectID": "downloading.html#downloading-sequencing-data",
    "href": "downloading.html#downloading-sequencing-data",
    "title": "downloading",
    "section": "",
    "text": "https://www.youtube.com/watch?v=fAaVzhD7DOU&ab_channel=WCBbioinformatics\nhttps://docs.google.com/presentation/d/12CJ-8CQSryc3K72kok3BHpw-rxTOv2pN06GJF2FP4xE/edit?usp=sharing\n\n\nPublished sequencing data is deposited online in\n\nSRA (Sequence Read Archive)\n\nThe SRA is a repository for raw sequencing data\n\nLong and short reads from any platform (Illumina, PacBio..)\n\nOrganised with unique identifiers\n\nSRP = Project/Study (1 or more samples )\nSRS = Sample (1 or more experiments)\nSRX = Experiment (1 or more runs i.e technical replicates)\nSRR = Run (results for 1 sequencing run)\n\nRuns stored in sra format\n\nReads saved in spots\n\nSmaller datasets and metadata can be downloaded via links in each run\nOther download options\n\nhttps://www.ncbi.nlm.nih.gov/sra/docs/sradownload/\nSRA Toolkit (see below)\nCloud access\n\n\nGEO (Gene Expression Omnibus)\n\nGenomics data repository for high throughput experiments\n\nExperimental metadata\nProcessed data e.g bigWig read profiles, ChIP-seq peak files, RNA-seq read count tables\nLinks to raw data (SRA)\nLinks to publication\n\nData organised into\n\nSeries and super series (GSE)\nSamples (GSM)\n\nMost publications will contain a GEO series ID.\nDownload links are available at the bottom of each GEO submission.\n\nTo download to the server\n\nRight click to “Copy link as”\nUse wget or curl on the command line to download using the URL\n\n\n\nENA (European Nucleotide Archive)\n\nENA is repository for sequencing data hosted by the European Bioinformatics Institute.\nTo download raw sequencing files from a project:\n\nSelect the fastq files you require.\nHit the “Get Download Script” button.\nRun the script on the terminal command line on your computer or on the server.\ne.g. sh ena-file-download-read-run-PRJNA112117-fastq_ftp-20240623-919.sh\n\n\n\n\n\n\n\n\n\n\nhttps://github.com/ncbi/sra-tools/wiki/08.-prefetch-and-fasterq-dump"
  },
  {
    "objectID": "customising.html",
    "href": "customising.html",
    "title": "Customising your command line experience",
    "section": "",
    "text": "An SSH key is an access credential, similar to a username/password that grants access to the server from a remote computer. You can set up ssh keys between your computer and the server to enable password-less login.\nFirstly create ssh key pairs (public and private) on your own machine (UNIX or Mac terminal):\nssh-keygen\nThis will ask for a pass-phrase. Just hit enter if you want to avoid entering this every time you login. You should use a pass-phrase if you need to protect your ssh keys from any who may have access to your computer.\nNext, copy the public key to the server:\nssh-copy-id username@bifx-core3.bio.ed.ac.uk\nThe next time you connect to the server, the SSH protocol will send a message to your computer and attempt to decrypt it with your private key. If it gets the expected response it will authenticate your connection:\nssh username@bifx-core3.bio.ed.ac.uk\nFurther information:\n\nHow to set up SSH keys\nSetting up SSH keys on Windows"
  },
  {
    "objectID": "customising.html#ssh-keys---logging-in-without-a-password",
    "href": "customising.html#ssh-keys---logging-in-without-a-password",
    "title": "Customising your command line experience",
    "section": "",
    "text": "An SSH key is an access credential, similar to a username/password that grants access to the server from a remote computer. You can set up ssh keys between your computer and the server to enable password-less login.\nFirstly create ssh key pairs (public and private) on your own machine (UNIX or Mac terminal):\nssh-keygen\nThis will ask for a pass-phrase. Just hit enter if you want to avoid entering this every time you login. You should use a pass-phrase if you need to protect your ssh keys from any who may have access to your computer.\nNext, copy the public key to the server:\nssh-copy-id username@bifx-core3.bio.ed.ac.uk\nThe next time you connect to the server, the SSH protocol will send a message to your computer and attempt to decrypt it with your private key. If it gets the expected response it will authenticate your connection:\nssh username@bifx-core3.bio.ed.ac.uk\nFurther information:\n\nHow to set up SSH keys\nSetting up SSH keys on Windows"
  },
  {
    "objectID": "customising.html#bash-configuration",
    "href": "customising.html#bash-configuration",
    "title": "Customising your command line experience",
    "section": "Bash configuration",
    "text": "Bash configuration\nComing soon…"
  },
  {
    "objectID": "customising.html#htop",
    "href": "customising.html#htop",
    "title": "Customising your command line experience",
    "section": "htop",
    "text": "htop\nhtop is a modified version of top for interactive system monitoring. It provides an overview of CPU and memory usage as well as running jobs on the server. You can find out more about how to use htop here: https://www.tecmint.com/htop-linux-process-monitoring/\nBy default htop will show the usage of each CPU on the machine. Because we have a lot of CPU’s this takes up the entire display making it very difficult to navigate. We recommend editing your htop configuration file ~/.config/htop/htopr to the parameters below. This will reduce the CPU usage to a single bar showing average usage and allow you to use htop as intended!\n# Beware! This file is rewritten by htop when settings are changed in the interface.\n# The parser is also very primitive, and not human-friendly.\nfields=0 48 17 18 38 39 40 2 46 47 49 1\nsort_key=46\nsort_direction=1\nhide_threads=0\nhide_kernel_threads=0\nhide_userland_threads=0\nshadow_other_users=0\nshow_thread_names=0\nshow_program_path=1\nhighlight_base_name=0\nhighlight_megabytes=0\nhighlight_threads=1\ntree_view=0\nheader_margin=0\ndetailed_cpu_time=0\ncpu_count_from_zero=0\nupdate_process_names=0\naccount_guest_in_cpu_meter=0\ncolor_scheme=0\ndelay=15\nleft_meters=CPU Memory Swap\nleft_meter_modes=1 1 1\nright_meters=CPU Tasks LoadAverage Uptime\nright_meter_modes=1 2 2 2"
  },
  {
    "objectID": "customising.html#tmux---terminal-multiplexer",
    "href": "customising.html#tmux---terminal-multiplexer",
    "title": "Customising your command line experience",
    "section": "TMUX - Terminal multiplexer",
    "text": "TMUX - Terminal multiplexer\nTMUX, the terminal multiplexer, is a program that allows you to manage persistent server sessions and easily switch between programs in one terminal session.\n\n“Detached” sessions that continue to run when you close your connection to the server.\nManage multiple sessions.\nSplit sessions into windows and panes.\n\nFurther resources:\n\ntmux cheatsheet\ntmux wiki"
  },
  {
    "objectID": "best_practices.html",
    "href": "best_practices.html",
    "title": "Best Practices",
    "section": "",
    "text": "This page contains a list of best practice documents for managing bioinformatics projects and working on the WCB servers.\n\nAccessing bioinformatics servers and user policy\nRunning and managing software on bifx-core servers\nBioinformatics workflow management tools\nCustomising your command line experience\nDownloading sequencing data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Running and managing software",
    "section": "",
    "text": "The command line tools available on the server can be classified as either standard GNU/Linux utilities or specialised bioinformatics tools.\n\n\nThe server provides access to a wide range of standard GNU/Linux utilities, which are installed in /usr/bin.\n\nA general introduction to the use of these utilities can be found here.\nA complete list of the available utilities can be obtained by running ls /usr/bin on the commmand line. More information on individual utilities can be accessed by running man [NAME OF UTILITY]\nA detailed manual covering many of the utilities available can be accessed by running the info command on the command line\n\n\n\n\nSpecialised bioinformatics tools are implemented on the server as singularity containers. The main benefit of this approach is that it allows multiple versions of the same tool to be installed in parallel on the server. This gives us a way of ensuring that the latest versions of software tools are available on the server without breaking existing user scripts and pipelines that rely on old versions. Each singularity container is accompanied by a wrapper script that enables users to call the corresponding tool directly. There are also two helper scripts: tools, which enables users to list the tools available, and versions, which lists the available versions of the tools. It is possible to run a tool by specifying its name as listed in the output of the tools command, or to run a specific version of a tool by including the version number as shown in the output of the versions command.\nWe recommend that you specify a particular version of any tools that are included in scripts. Calling the tool by name only generally runs to the latest version of the tool, which may change over time as new versions are installed. This could cause problems for scripts that were written to use older versions of the tool.\nThe man and info commands do not provide help for specialised bioinformatics tools on bifx-core. As an alternative, many tools provide help when run with the --help or -h flags, and provide version information when run with the --version or sometimes -version flags.\nSpecialised bioinformatics tools, or specific versions of tools, that are not currently installed on bifx-core can be requested by contacting the DRP-HCB bioinformatics core.\n\n\nIn this example, we show how to list and run different versions of bedtools on bifx-core:\nhyweldd@bifx-core3:~$ tools\nalignmentSieve\nbamCompare\nbamCoverage\nbamPEFragmentSize\nbedtools\n...\nhyweldd@bifx-core3:~$ bedtools --version\nbedtools v2.30.0\nhyweldd@bifx-core3:~$ bedtools --help\nbedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.30.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\n...\nhyweldd@bifx-core3:~$ versions bedtools\nbedtools-2.29.2\nbedtools-2.30.0\nhyweldd@bifx-core3:~$ bedtools-2.29.2 --version\nbedtools v2.29.2\nhyweldd@bifx-core3:~$ bedtools-2.30.0 --version\nbedtools v2.30.0\nhyweldd@bifx-core3:~$"
  },
  {
    "objectID": "software.html#running-command-line-tools",
    "href": "software.html#running-command-line-tools",
    "title": "Running and managing software",
    "section": "",
    "text": "The command line tools available on the server can be classified as either standard GNU/Linux utilities or specialised bioinformatics tools.\n\n\nThe server provides access to a wide range of standard GNU/Linux utilities, which are installed in /usr/bin.\n\nA general introduction to the use of these utilities can be found here.\nA complete list of the available utilities can be obtained by running ls /usr/bin on the commmand line. More information on individual utilities can be accessed by running man [NAME OF UTILITY]\nA detailed manual covering many of the utilities available can be accessed by running the info command on the command line\n\n\n\n\nSpecialised bioinformatics tools are implemented on the server as singularity containers. The main benefit of this approach is that it allows multiple versions of the same tool to be installed in parallel on the server. This gives us a way of ensuring that the latest versions of software tools are available on the server without breaking existing user scripts and pipelines that rely on old versions. Each singularity container is accompanied by a wrapper script that enables users to call the corresponding tool directly. There are also two helper scripts: tools, which enables users to list the tools available, and versions, which lists the available versions of the tools. It is possible to run a tool by specifying its name as listed in the output of the tools command, or to run a specific version of a tool by including the version number as shown in the output of the versions command.\nWe recommend that you specify a particular version of any tools that are included in scripts. Calling the tool by name only generally runs to the latest version of the tool, which may change over time as new versions are installed. This could cause problems for scripts that were written to use older versions of the tool.\nThe man and info commands do not provide help for specialised bioinformatics tools on bifx-core. As an alternative, many tools provide help when run with the --help or -h flags, and provide version information when run with the --version or sometimes -version flags.\nSpecialised bioinformatics tools, or specific versions of tools, that are not currently installed on bifx-core can be requested by contacting the DRP-HCB bioinformatics core.\n\n\nIn this example, we show how to list and run different versions of bedtools on bifx-core:\nhyweldd@bifx-core3:~$ tools\nalignmentSieve\nbamCompare\nbamCoverage\nbamPEFragmentSize\nbedtools\n...\nhyweldd@bifx-core3:~$ bedtools --version\nbedtools v2.30.0\nhyweldd@bifx-core3:~$ bedtools --help\nbedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.30.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\n...\nhyweldd@bifx-core3:~$ versions bedtools\nbedtools-2.29.2\nbedtools-2.30.0\nhyweldd@bifx-core3:~$ bedtools-2.29.2 --version\nbedtools v2.29.2\nhyweldd@bifx-core3:~$ bedtools-2.30.0 --version\nbedtools v2.30.0\nhyweldd@bifx-core3:~$"
  },
  {
    "objectID": "software.html#managing-your-own-software-tools-and-packages-with-conda",
    "href": "software.html#managing-your-own-software-tools-and-packages-with-conda",
    "title": "Running and managing software",
    "section": "Managing your own software tools and packages with conda",
    "text": "Managing your own software tools and packages with conda\nIn the preceding section, we have shown how to run command line tools that are installed on bifx-core. In addition to using these tools, it is also possible to manage your own software tools and environments using conda (see https://docs.conda.io/projects/conda/en/latest/ for more details). This is currently our recommended method for managing python packages and versions, which may be necessary if you are developing your own python scripts. While conda does provide the ability to manage R packages, we do not currently recommend using conda for this purpose. Our best practices for working with R packages are discussed later in this guide.\n\nInstalling conda\nIf you would like to use conda, we recommend that you install it in your home directory using the miniconda installer. You can do this using the following steps:\n\nDownload the miniconda installer into your home directory by navigating to your home directory and running wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nCalculate the sha256 checksum of the downloaded installer by running the command sha256sum Miniconda3-latest-Linux-x86_64.sh and check that it is the same as the value shown at https://repo.anaconda.com/miniconda/ for the Miniconda3-latest-Linux-x86_64.sh script\nIf the checksums match, install conda by running sh Miniconda3-latest-Linux-x86_64.sh and following the steps\n\n\nWhen asked about the license terms, review and accept them (type yes)\nAccept the offer to install conda to the default location /homes/[YOUR USERNAME]/miniconda3 by pressing ENTER\nAccept the offer to initialise miniconda3 by using conda init by typing yes\n\n\nWhen the installer has finished, log out from bifx-core3, then log back in and run conda --version to verify that conda has been installed\nRun conda update conda to ensure that conda is up to date\nRun conda config --set auto_activate_base false to prevent the base environment from being activated automatically on login\n\n\n\nSetting up your conda channels\nIn conda, software tools and packages are downloaded from remote repositories known as channels. conda provides its own default channels, but we recommend that users add the following two community mantained channels as well: - bioconda, which is dedicated to the distribution of bioinformatics software, and contains a far wider selection of bioinformatics tools than the default conda channels - conda-forge, which is a large community led channel that provides a wide range of software, including some tools that are required as dependencies by some bioinformatics tools distributed via the bioconda channel\nWe recommend that you add the bioconda channel with high priority, and the conda-forge channel with low priority, using the following steps: 1. Run conda config --add channels bioconda to add bioconda with high priority 2. Run conda config --append channels conda-forge to add conda-forge with low priority 3. Run conda config --show channels. You should see the following:\nchannels:\n    - bioconda\n    - defaults\n    - conda-forge\nNote: If the channels do not appear in the correct order, you can run conda config --remove channels bioconda, then conda config --remove channels conda-forge, then try again.\n\n\nManaging conda environments\nBy default, conda installs software tools into the directory in which it was installed, which is known as the base environment. You can check the location of this directory by running conda info. If you followed the steps above to install conda, this is the default location /homes/[YOUR USERNAME]/miniconda3.\nOne of the benefits of using conda to manage bioinformatics software is that it allows you to create different environments and easily switch between them. This is particularly useful if you work on multiple projects that use different bioinformatics software. Each user created environment has a dedicated directory stored in the env subdirectory of the base environment directory.\nWe recommend that you should always create your own conda environment to install software and leave the base environment as it is, even if you do not need multiple environments, as user environments are easier to modify and update than the base environment.\nThe following commands can be used to create and manage conda environments: - conda create -n [ENVIRONMENT NAME] [LIST OF PACKAGES TO INSTALL IN THE ENVIRONMENT (optional)] to create a new conda environment - conda activate [ENVIRONMENT NAME] to activate a conda environment - conda deactivate to deactivate a conda environment - conda env list to list the conda environments, showing which is active - conda remove to remove an environment - conda env export to export an environment to a yaml file - conda search to search for a software package - conda install to install a software package in the active environment\nA useful guide to conda commands can be found here:\nhttps://docs.conda.io/projects/conda/en/latest/user-guide/cheatsheet.html"
  },
  {
    "objectID": "software.html#using-python-on-bifx-core-servers",
    "href": "software.html#using-python-on-bifx-core-servers",
    "title": "Running and managing software",
    "section": "Using Python on bifx-core servers",
    "text": "Using Python on bifx-core servers\nWe recommend using Conda to maintain your Python packages and environments. JupyterHub can be accessed from within the Univeristy of Edinburgh network at the following url:\nhttps://bifx-core3.bio.ed.ac.uk:8888\nIf you would like to add a Conda environment to JupyterHub you will need to install nb_conda_kernels then add a pre-existing environment. You can run the following commands on the command line:\n## Install nb_conda_kernels in your conda base environment\nconda install nb_conda_kernels  # in base environment\n\n## Example to add the conda environment py3.9\npython -m ipykernel install --user --name py3.9 --display-name py3.9"
  },
  {
    "objectID": "software.html#using-r-on-bifx-core-servers",
    "href": "software.html#using-r-on-bifx-core-servers",
    "title": "Running and managing software",
    "section": "Using R on bifx-core servers",
    "text": "Using R on bifx-core servers\nWe recommend that you use RStudio to perform R analyses on the bifx-core servers. RStudio server is installed on bifx-core3, and can be accessed from within the University of Edinburgh network at the following url:\nhttps://bifx-rstudio.bio.ed.ac.uk\n\nManaging R packages with Renv\nRenv is a tool that is similar to conda, in that it makes it possible to create multiple environments, but is specialised for use with R, is implemented as an R package, and can be managed from the R console. You can learn more about Renv here:\nhttps://rstudio.github.io/renv/articles/renv.html"
  },
  {
    "objectID": "software.html#using-perl-on-bifx-core-servers",
    "href": "software.html#using-perl-on-bifx-core-servers",
    "title": "Running and managing software",
    "section": "Using perl on bifx-core servers",
    "text": "Using perl on bifx-core servers\nA number of different options are available for running perl scripts on bifx-core. We provide system wide installations that should be sufficient for most applications, and users also have the option of managing their own perl installations if they need specific perl versions or modules.\n\nSystem-wide perl installations on bifx-core servers\nThe system perl implementation on bifx-core, /usr/bin/perl, does not come with any extra packages, such as BioPerl packages, installed system wide. In order to run BioPerl scripts, we provide a custom containerised installation of perl, /library/software/bin/bioperl, that does include many of the BioPerl packages. This is our recommended version to use for most applications.\n\n\nManaging your own perl environment\nWhile we expect that /library/software/bin/bioperl should be sufficient for most bioinformatics applications, it may be necessary for some users to manage their own perl environments. We recommend the use of either conda or perlbrew for this purpose, as both allow users to manage local installations of perl without requiring administrator permissions.\nBoth conda and perlbrew have strengths and weaknesses relative to each other, so the choice of which to use depends on the user’s particular requirements.\nconda provides a quick and easy way of managing perl installations and environments, and can also be used to install other tools, as described earlier in this guide. However, when using conda to manage a perl installation, packages are installed using conda recipes rather than from CPAN. This may cause problems as not all perl packages have conda recipes available, and the conda recipe corresponding to a particular package may be difficult to find even when it exists.\nBy contrast perlbrew provides access to all CPAN packages through the cpan command, giving access to a wider range of packages. The main drawback of perlbrew when compared to conda is that it builds everything from source rather than providing pre-built packages. This makes the process of installing packages more time consuming, and also potentially more difficult if a particular CPAN package fails to build due to missing dependencies or failing tests. A further limitation of perlbrew when compared to conda is that it does not allow you to maintain different environments using the same version of perl.\nWe describe how to set up a local installation of perl using conda and perlbrew in the following sections.\n\nManaging your own perl installation with conda\nWe provide a conda environment containing the same packages as the environment used by /library/software/bin/bioperl in /library/software/conda_environments/BioPerl-1.7.2/BioPerl-1.7.2.yml. This can be used to add some additional packages, as in the following example (note that this example assumes that conda has already been installed, as described in the previous section).\n$ conda env create -f /library/software/conda_environments/BioPerl-1.7.2/BioPerl-1.7.2.yml\nCollecting package metadata (repodata.json): done\nSolving environment: done\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate bioperl-1.7.2\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n$ conda activate bioperl-1.7.2\n$ which perl\n~/miniconda3/envs/bioperl-1.7.2/bin/perl\n$ perl -E 'use DateTime; say \"Success!\"'\nCan't locate DateTime.pm in @INC (you may need to install the DateTime module) (@INC contains: /homes/hyweldd/miniconda3/envs/bioperl-1.7.2/lib/site_perl/5.26.2/x86_64-linux-thread-multi /homes/hyweldd/miniconda3/envs/bioperl-1.7.2/lib/site_perl/5.26.2 /homes/hyweldd/miniconda3/envs/bioperl-1.7.2/lib/5.26.2/x86_64-linux-thread-multi /homes/hyweldd/miniconda3/envs/bioperl-1.7.2/lib/5.26.2 .) at -e line 1.\nBEGIN failed--compilation aborted at -e line 1.\n$ conda search perl-datetime\nLoading channels: done\n# Name                       Version           Build  Channel\nperl-datetime                   1.42      pl5.22.0_0  bioconda\nperl-datetime                   1.42 pl526h2d50403_2  bioconda\n$ conda install perl-datetime\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n    \n...\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n$ perl -E 'use DateTime; say \"Success!\"'\nSuccess!\n$ \n\n\nManaging your own perl installation with perlbrew\nTo use perlbrew, you must first install it into your home directory. Once this is done, you can use the perlbrew command to install perl versions and download packages from CPAN for each version. The following example walks through the process of installing perlbrew, using it to install perl 5.32.1, and installing the DateTime package from CPAN using the cpan command. Further information on perlbrew can be found at https://perlbrew.pl.\n$ curl -L https://install.perlbrew.pl | bash\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   170  100   170    0     0    168      0  0:00:01  0:00:01 --:--:--   168\n100  1574  100  1574    0     0   1263      0  0:00:01  0:00:01 --:--:--  1263\n\n## Download the latest perlbrew\n\n## Installing perlbrew\nUsing Perl &lt;/usr/bin/perl&gt;\nperlbrew is installed: ~/perl5/perlbrew/bin/perlbrew\n\nperlbrew root (~/perl5/perlbrew) is initialized.\n\nAppend the following piece of code to the end of your ~/.bash_profile and start a\nnew shell, perlbrew should be up and fully functional from there:\n\n    source ~/perl5/perlbrew/etc/bashrc\n\nSimply run `perlbrew` for usage details.\n\nHappy brewing!\n\n## Installing patchperl\n\n## Done.\n$ echo 'source ~/perl5/perlbrew/etc/bashrc' &gt;&gt; ~/.bash_profile\n$ source ~/.bash_profile\n$ which perlbrew\n~/perl5/perlbrew/bin/perlbrew\n$ perlbrew --notest install 5.32.1\nInstalling /homes/hyweldd/perl5/perlbrew/build/perl-5.32.1/perl-5.32.1 into ~/perl5/perlbrew/perls/perl-5.32.1\n\nThis could take a while. You can run the following command on another shell to track the status:\n\n  tail -f ~/perl5/perlbrew/build.perl-5.32.1.log\n\nperl-5.32.1 is successfully installed.\n$ perlbrew use 5.32.1\n$ cpan DateTime\nLoading internal logger. Log::Log4perl recommended for better logging\nReading '/homes/hyweldd/.cpan/Metadata'\n  Database was generated on Fri, 04 Jun 2021 20:55:40 GMT\nRunning install for module 'DateTime'\nFetching with HTTP::Tiny:\nhttp://www.cpan.org/authors/id/D/DR/DROLSKY/DateTime-1.54.tar.gz\n\n...\n\n  DROLSKY/DateTime-1.54.tar.gz\n  /usr/bin/make install  -- OK\n$ perlbrew list-modules\n\n...\n\nDateTime\n\n...\n\n$ which perl\n~/perl5/perlbrew/perls/perl-5.32.1/bin/perl\n$ perl -v | grep version\nThis is perl 5, version 32, subversion 1 (v5.32.1) built for x86_64-linux\n$ perl -E 'use DateTime; say \"Success!\"'\nSuccess!\n$ \nNote: Due to the way the network is set up, installing BioPerl on bifx-core requires the NO_NETWORK_TESTING environment variable to be set, so the correct cpan command for installing BioPerl would be:\n$ NO_NETWORK_TESTING=1 cpan BioPerl\nFor further information, see https://github.com/libwww-perl/libwww-perl/issues/370."
  }
]